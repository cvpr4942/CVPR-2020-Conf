{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/processed/'\n",
    "back_dir = data_dir + 'background/'\n",
    "eval_dir = data_dir + 'evaluation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring reproducibility\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "percentage_split = False\n",
    "augment = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data\n",
    "\n",
    "The train data consists of 30 alphabets and 12 drawers.\n",
    "\n",
    "In each alphabet folder, there is 1 subfolder for each character in the alphabet. In each character subfolder, there are 20 images of this character, 1 for each of the 20 drawers.\n",
    " \n",
    "- To sample a like pair, first I randomly select an alphabet from the list of training alphabets (uniform proba). Then, I randomly select a character from the list of characters in this alphabet (uniform proba). Then, I randomly select 2 drawers from the list of training drawers (uniform proba) (note they can be the same) and select their corresponding image from the subdirectory.\n",
    "- To sample a dissimilar pair, first I randomly select 2 alphabets (uniform proba) (note they can be the same), then randomly select 2 drawers (uniform proba) (note they can be the same), then uniformly sample a character drawn by each of the two drawers. If the characters are the same, I redo the above step. Else, I load their corresponding image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 alphabets.\n"
     ]
    }
   ],
   "source": [
    "# get list of all alphabets\n",
    "background_alphabets = [os.path.join(back_dir, x) for x in next(os.walk(back_dir))[1]]\n",
    "background_alphabets.sort()\n",
    "\n",
    "# list of all drawers (1 to 20)\n",
    "background_drawers = list(np.arange(1, 21))\n",
    "\n",
    "print(\"There are {} alphabets.\".format(len(background_alphabets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 train-valid split\n",
    "if percentage_split:\n",
    "    valid_size = 0.2\n",
    "    num_alphabets = len(background_alphabets)\n",
    "\n",
    "    indices = list(range(num_alphabets))\n",
    "    split = int(np.floor(valid_size * num_alphabets))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_alphabets = [background_alphabets[idx] for idx in train_idx]\n",
    "    valid_alphabets = [background_alphabets[idx] for idx in valid_idx]\n",
    "\n",
    "    train_alphabets.sort()\n",
    "    valid_alphabets.sort()\n",
    "\n",
    "    # from 20 drawers, randomly select 12\n",
    "    train_drawers = list(np.random.choice(background_drawers, size=12, replace=False))\n",
    "    remaining_drawers = [x for x in background_drawers if x not in train_drawers]\n",
    "\n",
    "    print(\"There are {} train alphabets\".format(len(train_alphabets)))\n",
    "    print(\"There are {} valid alphabets\".format(len(valid_alphabets)))\n",
    "# 30 train, 10 valid\n",
    "else:\n",
    "    # from 40 alphabets, randomly select 30\n",
    "    train_alphabets = list(np.random.choice(background_alphabets, size=30, replace=False))\n",
    "    valid_alphabets = [x for x in background_alphabets if x not in train_alphabets]\n",
    "    \n",
    "    train_alphabets.sort()\n",
    "    valid_alphabets.sort()\n",
    "\n",
    "    # from 20 drawers, randomly select 12\n",
    "    train_drawers = list(np.random.choice(np.arange(20), size=12, replace=False))\n",
    "    remaining_drawers = [x for x in background_drawers if x not in train_drawers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_same_alph_character(filenames):\n",
    "    # same alphabet\n",
    "    if filenames[0].split('/')[4] == filenames[1].split('/')[4]:\n",
    "        # same character\n",
    "        if filenames[0].split('/')[5] == filenames[1].split('/')[5]:\n",
    "            return True\n",
    "        return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [02:46<00:00, 300.36it/s]\n"
     ]
    }
   ],
   "source": [
    "num_iters = int(100e3 / 2)\n",
    "\n",
    "same_drawer = 0\n",
    "selected_alphabets = []\n",
    "redoing = 0\n",
    "img_pairs = []\n",
    "label_pairs = []\n",
    "for i in trange(num_iters):\n",
    "    # sample a like pair\n",
    "    if i % 2 == 0:\n",
    "        # uniformly select 1 alphabet\n",
    "        alph = np.random.choice(train_alphabets)\n",
    "        selected_alphabets.append(alph)\n",
    "                \n",
    "        # uniformly sample 1 character\n",
    "        chars = [os.path.join(alph, x) for x in next(os.walk(alph))[1]]\n",
    "        char = np.random.choice(chars)\n",
    "                \n",
    "        # uniformly sample 2 drawers\n",
    "        ds = np.random.choice(train_drawers, size=2, replace=True)\n",
    "                \n",
    "        # get list of filenames to read in char dir\n",
    "        filenames = [\n",
    "            os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "            ) in ds\n",
    "        ]\n",
    "        \n",
    "        # in case I get the same drawer\n",
    "        if len(filenames) == 1:\n",
    "            same_drawer += 1\n",
    "            filenames = filenames * 2\n",
    "        \n",
    "        # load pair as numpy array and store\n",
    "        pair = []\n",
    "        for name in filenames:\n",
    "            img_arr = img2array(name, gray=True, expand=True)\n",
    "            img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "            pair.append(img_arr)        \n",
    "        img_pairs.append(np.concatenate(pair, axis=0))\n",
    "        \n",
    "        # store ground truth lbl\n",
    "        gd_truth = np.array([1], dtype=np.int64)\n",
    "        label_pairs.append(gd_truth)\n",
    "        \n",
    "    # sample a dissimilar pair\n",
    "    else:\n",
    "        redo = True\n",
    "        while redo:\n",
    "            # uniformly select 2 alphabets\n",
    "            alph = np.random.choice(train_alphabets, size=2, replace=True)\n",
    "            selected_alphabets.extend(alph)\n",
    "\n",
    "            # uniformly sample 2 drawers\n",
    "            ds = np.random.choice(train_drawers, size=2, replace=True)\n",
    "\n",
    "            filenames = []\n",
    "            for i, a in enumerate(alph):\n",
    "                # uniformly sample 1 character\n",
    "                chars = [os.path.join(a, x) for x in next(os.walk(a))[1]]\n",
    "                char = np.random.choice(chars)\n",
    "\n",
    "                # get list of filenames to read in char dir\n",
    "                name = [\n",
    "                    os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                        x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "                    ) == ds[i]\n",
    "                ]\n",
    "                filenames.append(*name)\n",
    "            \n",
    "            # reject (same alph, same char, same drawer) and (same alph, same char, diff drawer)\n",
    "            # i.e. just check if same alph and same char\n",
    "            redo = check_same_alph_character(filenames)\n",
    "            if redo:\n",
    "                redoing += 1\n",
    "\n",
    "        # load pair as numpy array and store\n",
    "        pair = []\n",
    "        for name in filenames:\n",
    "            img_arr = img2array(name, gray=True, expand=True)\n",
    "            img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "            pair.append(img_arr)        \n",
    "        img_pairs.append(np.concatenate(pair, axis=0))\n",
    "\n",
    "         # store ground truth lbl\n",
    "        gd_truth = np.array([0], dtype=np.int64)\n",
    "        label_pairs.append(gd_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redid 29 false pairs...\n",
      "Selected the same drawer 2018 times...\n"
     ]
    }
   ],
   "source": [
    "print(\"Redid {} false pairs...\".format(redoing))\n",
    "print(\"Selected the same drawer {} times...\".format(same_drawer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'./data/processed/background/Angelic': 2422,\n",
       "         './data/processed/background/Anglo-Saxon_Futhorc': 2531,\n",
       "         './data/processed/background/Armenian': 2470,\n",
       "         './data/processed/background/Asomtavruli_(Georgian)': 2571,\n",
       "         './data/processed/background/Atlantean': 2467,\n",
       "         './data/processed/background/Aurek-Besh': 2548,\n",
       "         './data/processed/background/Balinese': 2591,\n",
       "         './data/processed/background/Bengali': 2512,\n",
       "         './data/processed/background/Blackfoot_(Canadian_Aboriginal_Syllabics)': 2570,\n",
       "         './data/processed/background/Braille': 2520,\n",
       "         './data/processed/background/Burmese_(Myanmar)': 2473,\n",
       "         './data/processed/background/Cyrillic': 2566,\n",
       "         './data/processed/background/Early_Aramaic': 2495,\n",
       "         './data/processed/background/Futurama': 2526,\n",
       "         './data/processed/background/Ge_ez': 2544,\n",
       "         './data/processed/background/Grantha': 2426,\n",
       "         './data/processed/background/Gujarati': 2527,\n",
       "         './data/processed/background/Inuktitut_(Canadian_Aboriginal_Syllabics)': 2485,\n",
       "         './data/processed/background/Japanese_(hiragana)': 2474,\n",
       "         './data/processed/background/Japanese_(katakana)': 2435,\n",
       "         './data/processed/background/Kannada': 2486,\n",
       "         './data/processed/background/Keble': 2448,\n",
       "         './data/processed/background/Korean': 2497,\n",
       "         './data/processed/background/Malay_(Jawi_-_Arabic)': 2485,\n",
       "         './data/processed/background/Mkhedruli_(Georgian)': 2492,\n",
       "         './data/processed/background/N_Ko': 2556,\n",
       "         './data/processed/background/Ojibwe_(Canadian_Aboriginal_Syllabics)': 2508,\n",
       "         './data/processed/background/Sanskrit': 2489,\n",
       "         './data/processed/background/Syriac_(Estrangelo)': 2455,\n",
       "         './data/processed/background/Tifinagh': 2489})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that each alphabet gets approximately equal representation\n",
    "Counter(selected_alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle img and labels to prevent monotone (same, different) sequence\n",
    "indices = list(range(len(img_pairs)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "img_pairs = [img_pairs[idx] for idx in indices]\n",
    "label_pairs = [label_pairs[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(img_pairs, data_dir + 'X_train.p')\n",
    "pickle_dump(label_pairs, data_dir + 'y_train.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data Augmentation\n",
    "\n",
    "We add 8 transforms for each training example pair in `img_pair`. The transformation is affine with the following constraints on its parameters:\n",
    "\n",
    "- theta $\\in$ [-10, 10] uniform (rotation)\n",
    "- $\\rho_x$ and $\\rho_y$ $\\in$ [-0.3, 0.3] uniform (shear)\n",
    "- $s_x$ and $s_y$ $\\in$ [0.8, 1.2] uniform (scale)\n",
    "- $t_x$ and $t_y$ $\\in$ [-2, 2] uniform (translation)\n",
    "\n",
    "Each of these parameters is included with probability 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2pil = transforms.ToPILImage()\n",
    "\n",
    "def pil2array(im):\n",
    "    x = np.asarray(im, dtype=np.float32)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255\n",
    "    return x\n",
    "\n",
    "augmented_img_pairs = []\n",
    "augmented_label_pairs = []\n",
    "for idx in trange(len(img_pairs)):\n",
    "    # get gd truth label\n",
    "    label = label_pairs[idx]\n",
    "    \n",
    "    # grab img pair\n",
    "    pair = img_pairs[idx]\n",
    "    pair = np.transpose(pair, (0, 2, 3, 1))\n",
    "    im1, im2 = np.array(pair)\n",
    "    \n",
    "    # convert back to [0, 255] range\n",
    "    im1 *= 255\n",
    "    im2 *= 255\n",
    "    \n",
    "    # transform to PIL image\n",
    "    im1, im2 = arr2pil(im1), arr2pil(im2)\n",
    "    \n",
    "    # compose 8 transforms\n",
    "    for i in range(8):\n",
    "        # randomly select transform with proba 0.5\n",
    "        rot = random.choice([0, [-10, 10]])\n",
    "        shear = random.choice([None, [-0.3, 0.3]])\n",
    "        scale = random.choice([None, [0.8, 1.2]])\n",
    "        trans = random.choice([None, [2/150, 2/150]]) # absolute value\n",
    "        \n",
    "        # apply affine transformation\n",
    "        aff = transforms.RandomAffine(rot, trans, scale, shear)\n",
    "        aug_im1, aug_im2 = aff(im1), aff(im2)\n",
    "        \n",
    "        # convert to numpy array\n",
    "        aug_im1 = pil2array(aug_im1)\n",
    "        aug_im2 = pil2array(aug_im2)\n",
    "        \n",
    "        # transpose to C,H,W\n",
    "        aug_im1 = np.transpose(aug_im1, (0, 3, 1, 2))\n",
    "        aug_im2 = np.transpose(aug_im2, (0, 3, 1, 2))\n",
    "        \n",
    "        # add to list\n",
    "        aug_pairs = np.concatenate([aug_im1, aug_im2], axis=0)\n",
    "        augmented_img_pairs.append(aug_pairs)\n",
    "        augmented_label_pairs.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Train and Augmented Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle img and labels\n",
    "indices = list(range(len(augmented_img_pairs)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "augmented_img_pairs = [augmented_img_pairs[idx] for idx in indices]\n",
    "augmented_label_pairs = [augmented_label_pairs[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_pairs = img_pairs + augmented_img_pairs\n",
    "train_label_pairs = label_pairs + augmented_label_pairs\n",
    "\n",
    "print(\"Effective Train Size: {}\".format(2 * len(train_img_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(train_img_pairs, data_dir + 'X_train_aug.p')\n",
    "pickle_dump(train_label_pairs, data_dir + 'y_train_aug.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data\n",
    "\n",
    "The validation data consists of 10 alphabets and 4 drawers.\n",
    "\n",
    "The authors used 2 types of validation strategies for early-stopping of the model training. One of the method consists in creating a validation one-shot scenario to test the model's ability to generalize. We pick an alphabet from among the 10 available, choose 16 characters uniformly at random and select 2 of the 4 available drawers. We then select all the 16 characters produced by the first drawer, and individually compare against all 16 characters from the second drawer, with the goal of predicting the class of the character from among all of the second drawer's characters.\n",
    "\n",
    "This process is repeated twice for all alphabets (the second time we pick the 2 other drawers), so that there are 32 one-shot learning trials for each of the 10 validation alphabets, for a total of 320 one-shot trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from remaining 8 drawers, select 4\n",
    "valid_drawers = list(np.random.choice(remaining_drawers, size=4, replace=False))\n",
    "remaining_drawers = [x for x in remaining_drawers if x not in valid_drawers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "10/10\n"
     ]
    }
   ],
   "source": [
    "num_iters = len(valid_alphabets)\n",
    "\n",
    "# number of characters to sample in each alphabet\n",
    "pop = 14\n",
    "\n",
    "valid_img_pairs = []\n",
    "valid_label_pairs = []\n",
    "for iter, alph in enumerate(valid_alphabets):\n",
    "    print(\"{}/{}\".format(iter+1, num_iters))\n",
    "    for j in range(2):\n",
    "        # grab drawers\n",
    "        ds = [valid_drawers[2*j], valid_drawers[2*j + 1]]\n",
    "        \n",
    "        # sample pop characters uniformly\n",
    "        chars = [os.path.join(alph, x) for x in next(os.walk(alph))[1]]\n",
    "        chars = np.random.choice(chars, size=pop, replace=False)\n",
    "        \n",
    "        # grab filenames for both drawers\n",
    "        filenames = []\n",
    "        for d in ds:\n",
    "            for char in chars:\n",
    "                names = [\n",
    "                    os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                        x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "                    ) == d\n",
    "                ]\n",
    "                filenames.append(*names)\n",
    "        d1 = filenames[:pop]\n",
    "        d2 = filenames[pop:]\n",
    "        \n",
    "        \n",
    "        for i, left in enumerate(d1):\n",
    "            way_pairs = []\n",
    "            way_labels = []\n",
    "            for right in d2:\n",
    "                img_names = [left, right]\n",
    "                pair = []\n",
    "                for name in img_names:\n",
    "                    img_arr = img2array(name, gray=True, expand=True)\n",
    "                    img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "                    pair.append(img_arr) \n",
    "                # create img and store\n",
    "                pair = np.concatenate(pair, axis=0)\n",
    "                way_pairs.append(pair)\n",
    "            \n",
    "            # create pop-way task\n",
    "            way_pairs = [np.expand_dims(x, axis=0) for x in way_pairs]\n",
    "            way_pairs = np.concatenate(way_pairs, axis=0)\n",
    "            valid_img_pairs.append(way_pairs)\n",
    "            label = np.array([i], dtype=np.int64)\n",
    "            valid_label_pairs.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 345758400 total bytes\n",
      "Writing bytes [0, 345758400]\n",
      "Done!\n",
      "Writing 10769 total bytes\n",
      "Writing bytes [0, 10769]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pickle_dump(valid_img_pairs, data_dir + 'X_valid.p')\n",
    "pickle_dump(valid_label_pairs, data_dir + 'y_valid.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "The test set consists in 10 alphabets and 4 drawers (just like the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of alphabets\n",
    "test_alphabets = [os.path.join(eval_dir, x) for x in next(os.walk(eval_dir))[1]]\n",
    "test_alphabets.sort()\n",
    "\n",
    "# there are 20 drawers\n",
    "test_drawers = remaining_drawers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "10/10\n"
     ]
    }
   ],
   "source": [
    "num_iters = len(test_alphabets)\n",
    "\n",
    "# number of characters to sample in each alphabet\n",
    "pop = 20\n",
    "\n",
    "test_img_pairs = []\n",
    "test_label_pairs = []\n",
    "for iter, alph in enumerate(test_alphabets):\n",
    "    print(\"{}/{}\".format(iter+1, num_iters))\n",
    "    for j in range(2):\n",
    "        # sample a pair of drawers\n",
    "        ds = np.random.choice(test_drawers, size=2, replace=False)\n",
    "        \n",
    "        # sample 20 characters uniformly\n",
    "        chars = [os.path.join(alph, x) for x in next(os.walk(alph))[1]]\n",
    "        chars = np.random.choice(chars, size=pop, replace=False)\n",
    "        \n",
    "        # grab filenames for both drawers\n",
    "        filenames = []\n",
    "        for d in ds:\n",
    "            for char in chars:\n",
    "                names = [\n",
    "                    os.path.join(char, x) for x in next(os.walk(char))[-1] if int(\n",
    "                        x.split(\"_\")[1][0:2].lstrip(\"0\")\n",
    "                    ) == d\n",
    "                ]\n",
    "                filenames.append(*names)\n",
    "        d1 = filenames[:pop]\n",
    "        d2 = filenames[pop:]\n",
    "\n",
    "        for i, left in enumerate(d1):\n",
    "            way_pairs = []\n",
    "            way_labels = []\n",
    "            for right in d2:\n",
    "                img_names = [left, right]\n",
    "                pair = []\n",
    "                for name in img_names:\n",
    "                    img_arr = img2array(name, gray=True, expand=True)\n",
    "                    img_arr = np.transpose(img_arr, (0, 3, 1, 2))\n",
    "                    pair.append(img_arr) \n",
    "               # create img and store\n",
    "                pair = np.concatenate(pair, axis=0)\n",
    "                way_pairs.append(pair)\n",
    "                \n",
    "            # create pop-way task\n",
    "            way_pairs = [np.expand_dims(x, axis=0) for x in way_pairs]\n",
    "            way_pairs = np.concatenate(way_pairs, axis=0)\n",
    "            test_img_pairs.append(way_pairs)\n",
    "            label = np.array([i], dtype=np.int64)\n",
    "            test_label_pairs.append(label)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 705620520 total bytes\n",
      "Writing bytes [0, 705620520]\n",
      "Done!\n",
      "Writing 15329 total bytes\n",
      "Writing bytes [0, 15329]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pickle_dump(test_img_pairs, data_dir + 'X_test.p')\n",
    "pickle_dump(test_label_pairs, data_dir + 'y_test.p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
